- liste de requêtes prédéfinies (exemple sur date de création de fichier en cohérence avec le SVN : alors range "gte": "01/01/2016")
cela permettrait de récupérer tous les derniers fichiers modifiés sur le SVN

- Corrections sur l'affichage des fragments d'aperçus de fichiers et mise en forme
- fonctions d'échappement des caractères html angularjs "escapeall" (seulement pour le cas des findAll durant lequel le contenu est renvoyé brut,
et ce n'est pas un highlight formatté par elasticsearch et le mode highlight encode html)

## Mettre les filtres en exclusions
exemple, par un ctrl+clic sur un filtre de version, afficher en rouge la version exclue, et lancer la recherche sur toutes les versions sauf celles exclues
ou alors, par un bouton switch on/off si le filtre agit en inclusion/exclusion

SVN :
utilisation du paramètre : -Dsvnkit.http.spoolDirectory=./spooldirectory
permet de réaliser un checkout temporaire et éviter les soucis de E175002: REPORT request failed on '/svn/!svn/vcc/default'


*********** page d'aide ************
les wildcards et joker (* / ?)
les champs à utiliser lastDate, lastAuthor, name, path, version, project, size
les dates :
    lastDate:<2010-01-01 AND lastDate:>2005-01-01
    ou mieux
    lastDate:[2012-01-01 2016-01-01]
    en filtrant avec l'heure précise : lastDate:[2016-01-01T00:00:00 2016-01-04T10:00:00]
mais encore :
lastDate:[now-12h now]
lastDate:[now-12h *]
************************************


1)fonctionnalitée à inclure
- inclure un moteur de recherche intégré à la page qui s'enregistre dans le navigateur, avec un raccourci clavier (k ?)
- le crate https://docs.rs/croner/latest/croner/ a l'air mieux que cron

2)filtre sur date de modif

## Crawler
 - dans le crawler, si le last_modified n'a pas bougé, alors on ne réindexe pas le fichier en question
 - pouvoir parcourir dans le crawler directement le SVN / Git (voir point 1)
 - gérer l'accès en JWT à l'api crawler (utiliser le user system ?) voir : https://github.com/Gleetr/auth0-curl/blob/master/curl-auth0

####IDEE A PENSER####
Faire des filtres directement sur les entêtes des colonnes (on pourrait par exemple imaginer que "extension" a un filtre "java" et version a un filtre "branche1"
cela permettra d'éviter d'écrire directement dans le champ de recherche pour les non initiés des requêtes du genre "version:branche1 AND extension:java"


- quand, on réindex un document existant, comment on fait, date, est-ce qu'il est bien supprimé d'une version etc, ou tout court ?
- enregistrer le document une seule fois dans l'index, et enregistrer une liste des versions, des paths, des projets, si le contenu est le même ?

Gros problème : si un crawl immense n'était pas fini et que le serveur redémarre, le serveur reprend le crawling au démarrage, mais il ne répond à aucune autre requêtes => IMPOSSIBLE de se logguer !


- pour le crawling github, il faut décrire que l'access token doit être de type "All repositories"
et les permissions Contents en readonly et Metadata
- le mot de passe oauth est dans le remote github, pas top secu, il faut utiliser les credentials helpers comme avant
- il faudrait gérer les erreurs de crawling sur l'interface, par exemple vpn pas monté, accès réseau down
- dans la configuration de l'index, on devrait pouvoir setter du tuning dessus, exemple, 10MB max par fichier, liste des extensions, tuning fin mémoire pour tantivy, à rajouter peut-être un onglet settings
- parti profile du user à implémenter
- sécuriser l'accès à l'API backend
- on gère les exclude, mais ce serait mieux de gérer aussi les includes
_ parfois j'ai INFO tantivy::indexer::index_writer: Buffer limit reached, flushing segment with maxdoc=1484.
- on flush pas régulièrement comme avant, lors des longs crawl c'est problèmatique
- WARN sqlx::pool::acquire: acquired connection, but time to acquire exceeded slow threshold aquired_after_secs=7.390929384 slow_acquire_threshold_secs=2.0
- cargo tree -e no-dev montre beaucoup de dépendances gix, à voir avec les features si on peut réduire le champ des dépendances